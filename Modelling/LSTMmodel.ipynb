{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI model for predicting the electricity prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "from LSTMmodel import *\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to GPU if available (Metal)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpotPriceDKK</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>temp_Aabenraa</th>\n",
       "      <th>temp_Aalborg</th>\n",
       "      <th>temp_Aarhus</th>\n",
       "      <th>temp_Assens</th>\n",
       "      <th>temp_Billund</th>\n",
       "      <th>temp_Brønderslev</th>\n",
       "      <th>temp_Esbjerg</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed_Thisted</th>\n",
       "      <th>wind_speed_Tønder</th>\n",
       "      <th>wind_speed_Varde</th>\n",
       "      <th>wind_speed_Vejen</th>\n",
       "      <th>wind_speed_Vejle</th>\n",
       "      <th>wind_speed_Vesthimmerlands</th>\n",
       "      <th>wind_speed_Viborg</th>\n",
       "      <th>wind_speed_Ærø</th>\n",
       "      <th>natural_gas</th>\n",
       "      <th>crude_oil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120.080002</td>\n",
       "      <td>2024-11-25 23:00:00+00:00</td>\n",
       "      <td>2024-11-26 00:00:00+00:00</td>\n",
       "      <td>8.8</td>\n",
       "      <td>7.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>8.5</td>\n",
       "      <td>7.8</td>\n",
       "      <td>9.2</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9</td>\n",
       "      <td>6.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5.9</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.78</td>\n",
       "      <td>74.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298.309998</td>\n",
       "      <td>2024-11-25 22:00:00+00:00</td>\n",
       "      <td>2024-11-25 23:00:00+00:00</td>\n",
       "      <td>8.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>9.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>9.2</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.9</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.78</td>\n",
       "      <td>74.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197.039993</td>\n",
       "      <td>2024-11-25 21:00:00+00:00</td>\n",
       "      <td>2024-11-25 22:00:00+00:00</td>\n",
       "      <td>9.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>8.4</td>\n",
       "      <td>9.4</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.78</td>\n",
       "      <td>74.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227.539993</td>\n",
       "      <td>2024-11-25 20:00:00+00:00</td>\n",
       "      <td>2024-11-25 21:00:00+00:00</td>\n",
       "      <td>9.4</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.5</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.78</td>\n",
       "      <td>74.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>289.880005</td>\n",
       "      <td>2024-11-25 19:00:00+00:00</td>\n",
       "      <td>2024-11-25 20:00:00+00:00</td>\n",
       "      <td>9.7</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.7</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.78</td>\n",
       "      <td>74.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 473 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SpotPriceDKK                       from                         to  \\\n",
       "0    120.080002  2024-11-25 23:00:00+00:00  2024-11-26 00:00:00+00:00   \n",
       "1    298.309998  2024-11-25 22:00:00+00:00  2024-11-25 23:00:00+00:00   \n",
       "2    197.039993  2024-11-25 21:00:00+00:00  2024-11-25 22:00:00+00:00   \n",
       "3    227.539993  2024-11-25 20:00:00+00:00  2024-11-25 21:00:00+00:00   \n",
       "4    289.880005  2024-11-25 19:00:00+00:00  2024-11-25 20:00:00+00:00   \n",
       "\n",
       "   temp_Aabenraa  temp_Aalborg  temp_Aarhus  temp_Assens  temp_Billund  \\\n",
       "0            8.8           7.8          8.0          8.8           8.5   \n",
       "1            8.9           8.0          8.3          9.1           8.6   \n",
       "2            9.1           8.5          8.6          9.3           8.8   \n",
       "3            9.4           8.9          9.1          9.6           9.0   \n",
       "4            9.7           9.6          9.6          9.8           9.4   \n",
       "\n",
       "   temp_Brønderslev  temp_Esbjerg  ...  wind_speed_Thisted  wind_speed_Tønder  \\\n",
       "0               7.8           9.2  ...                10.9                6.8   \n",
       "1               7.9           9.2  ...                10.3                6.5   \n",
       "2               8.4           9.4  ...                10.2                6.5   \n",
       "3               8.9           9.5  ...                10.0                6.5   \n",
       "4               9.6           9.7  ...                 9.2                7.3   \n",
       "\n",
       "   wind_speed_Varde  wind_speed_Vejen  wind_speed_Vejle  \\\n",
       "0               8.0               6.7               5.5   \n",
       "1               7.9               6.5               5.7   \n",
       "2               7.6               6.6               5.7   \n",
       "3               7.5               6.3               5.6   \n",
       "4               7.6               7.1               5.8   \n",
       "\n",
       "   wind_speed_Vesthimmerlands  wind_speed_Viborg  wind_speed_Ærø  natural_gas  \\\n",
       "0                         6.9                5.9             7.7         2.78   \n",
       "1                         6.3                5.5             7.9         2.78   \n",
       "2                         6.4                5.7             7.8         2.78   \n",
       "3                         6.6                6.0             7.8         2.78   \n",
       "4                         6.8                6.7             7.4         2.78   \n",
       "\n",
       "   crude_oil  \n",
       "0      74.27  \n",
       "1      74.27  \n",
       "2      74.27  \n",
       "3      74.27  \n",
       "4      74.27  \n",
       "\n",
       "[5 rows x 473 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75864, 473)\n"
     ]
    }
   ],
   "source": [
    "# path to the CSV file\n",
    "path = '../../data/fulldata.csv'\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "display(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpotPriceDKK</th>\n",
       "      <th>from</th>\n",
       "      <th>temp_Aabenraa</th>\n",
       "      <th>temp_Aalborg</th>\n",
       "      <th>temp_Aarhus</th>\n",
       "      <th>temp_Assens</th>\n",
       "      <th>temp_Billund</th>\n",
       "      <th>temp_Brønderslev</th>\n",
       "      <th>temp_Esbjerg</th>\n",
       "      <th>temp_Faaborg-Midtfyn</th>\n",
       "      <th>...</th>\n",
       "      <th>natural_gas_-72</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75791</th>\n",
       "      <td>149.070007</td>\n",
       "      <td>2016-04-05 00:00:00+00:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75790</th>\n",
       "      <td>147.580002</td>\n",
       "      <td>2016-04-05 01:00:00+00:00</td>\n",
       "      <td>6.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75789</th>\n",
       "      <td>149.520004</td>\n",
       "      <td>2016-04-05 02:00:00+00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75788</th>\n",
       "      <td>158.300003</td>\n",
       "      <td>2016-04-05 03:00:00+00:00</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75787</th>\n",
       "      <td>170.289993</td>\n",
       "      <td>2016-04-05 04:00:00+00:00</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SpotPriceDKK                      from  temp_Aabenraa  temp_Aalborg  \\\n",
       "75791    149.070007 2016-04-05 00:00:00+00:00            6.1           4.6   \n",
       "75790    147.580002 2016-04-05 01:00:00+00:00            6.8           4.1   \n",
       "75789    149.520004 2016-04-05 02:00:00+00:00            7.0           3.4   \n",
       "75788    158.300003 2016-04-05 03:00:00+00:00            7.8           2.7   \n",
       "75787    170.289993 2016-04-05 04:00:00+00:00            8.8           2.1   \n",
       "\n",
       "       temp_Aarhus  temp_Assens  temp_Billund  temp_Brønderslev  temp_Esbjerg  \\\n",
       "75791          4.6          5.5           3.9               4.4           5.2   \n",
       "75790          4.0          5.5           3.5               3.8           5.1   \n",
       "75789          3.6          5.2           4.0               3.2           5.2   \n",
       "75788          3.6          5.5           4.8               2.5           6.2   \n",
       "75787          3.9          6.7           6.8               1.9           7.3   \n",
       "\n",
       "       temp_Faaborg-Midtfyn  ...  natural_gas_-72  hour  day  month  hour_sin  \\\n",
       "75791                   5.4  ...             1.88     0    1      4  0.000000   \n",
       "75790                   5.0  ...             1.88     1    1      4  0.258819   \n",
       "75789                   4.9  ...             1.88     2    1      4  0.500000   \n",
       "75788                   5.1  ...             1.88     3    1      4  0.707107   \n",
       "75787                   6.0  ...             1.88     4    1      4  0.866025   \n",
       "\n",
       "       hour_cos   day_sin  day_cos  month_sin  month_cos  \n",
       "75791  1.000000  0.781831  0.62349   0.866025       -0.5  \n",
       "75790  0.965926  0.781831  0.62349   0.866025       -0.5  \n",
       "75789  0.866025  0.781831  0.62349   0.866025       -0.5  \n",
       "75788  0.707107  0.781831  0.62349   0.866025       -0.5  \n",
       "75787  0.500000  0.781831  0.62349   0.866025       -0.5  \n",
       "\n",
       "[5 rows x 495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75768, 495)\n"
     ]
    }
   ],
   "source": [
    "data['from'] = pd.to_datetime(data['from'])\n",
    "\n",
    "# drop to column\n",
    "data = data.drop(columns=['to'])\n",
    "\n",
    "# explicitly including lagged prices as well\n",
    "lag_hours = [0, \n",
    "            -1, -2, -3, -4, -5, -6, -24, #-48, -72, -96, -120, -144, -168\n",
    "             ]\n",
    "for lag in lag_hours:\n",
    "    data[f'price_lag_{lag}'] = data['SpotPriceDKK'].shift(lag)\n",
    "\n",
    "lag_oil_gas = [-24, -48, -72, # -168\n",
    "               ]\n",
    "for lag in lag_oil_gas:\n",
    "    data[f'crude_oil_{lag}'] = data['crude_oil'].shift(lag)\n",
    "    data[f'natural_gas_{lag}'] = data['natural_gas'].shift(lag)\n",
    "\n",
    "# offset price by 1 day\n",
    "data['SpotPriceDKK'] = data['SpotPriceDKK'].shift(24)\n",
    "# drop the first 24 rows\n",
    "data = data.dropna()\n",
    "\n",
    "# offset the from date by 1 day to match the price\n",
    "data['from'] = data['from'] + pd.DateOffset(days=1)\n",
    "\n",
    "# cyclical encoding for hour, day, and month\n",
    "data['hour'] = data['from'].dt.hour\n",
    "data['day'] = data['from'].dt.dayofweek\n",
    "data['month'] = data['from'].dt.month\n",
    "\n",
    "cyclical_encoding(data,'hour',24)\n",
    "cyclical_encoding(data,'day',7)\n",
    "cyclical_encoding(data,'month',12)\n",
    "\n",
    "# sort data to be ascending\n",
    "data = data.sort_values('from')\n",
    "\n",
    "display(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of features before interaction terms: 493\n",
      "Target variable: ['SpotPriceDKK']\n"
     ]
    }
   ],
   "source": [
    "# extract column names\n",
    "cols = data.columns.tolist()\n",
    "\n",
    "# select features and target variable\n",
    "all_features = cols[2:]\n",
    "target = cols[:1]\n",
    "\n",
    "print(f'Count of features before interaction terms: {len(all_features)}')\n",
    "print(f'Target variable: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data: until July 2023\n",
    "train_data = data[data['from'] < '2023-08-01']\n",
    "# train_data = data[(data['from'] >= '2016-04-01') & (data['from'] < '2023-08-01')]\n",
    "\n",
    "# validation data: July 2023 to December 2023\n",
    "val_data = data[(data['from'] >= '2023-08-01') & (data['from'] < '2024-08-01')]\n",
    "\n",
    "# Test data: 2024 and beyond\n",
    "test_data = data[data['from'] >= '2024-08-01']\n",
    "\n",
    "X_train = train_data[all_features].values\n",
    "X_val = val_data[all_features].values\n",
    "X_test = test_data[all_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# fit the scaler on the training features and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the validation and test features using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# fit the PCA on the training features and transform\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# transform the validation and test features using the same PCA\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target values\n",
    "train_targets = train_data[target].values\n",
    "val_targets = val_data[target].values\n",
    "test_targets = test_data[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "torch.manual_seed(2024)\n",
    "np.random.seed(2024)\n",
    "\n",
    "# hyperparameters\n",
    "seq_length = 24  # Use past 24 hours to form a sequence\n",
    "batch_size = 256\n",
    "input_dim = X_train_pca.shape[1]\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# hidden_dim = 256\n",
    "# layer_dim = 2\n",
    "# # regularization parameter\n",
    "# lambda_l1 = 1 # Adjust based on desired regularization strength\n",
    "# lambda_l2 = 1e-5 # Adjust based on desired regularization strength\n",
    "\n",
    "\n",
    "# arrays for tuning\n",
    "lambda_l1_array = [1.5, 1, 1e-1]\n",
    "lambda_l2_array = [1e-3, 1e-4, 1e-5]\n",
    "hidden_dim_array = [256, 512]\n",
    "layer_dim_array = [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = dataset(X_train_pca, train_targets, seq_length)\n",
    "val_dataset = dataset(X_val_pca, val_targets, seq_length)\n",
    "test_dataset = dataset(X_test_pca, test_targets, seq_length)\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# # initialize the model\n",
    "# model = LSTMmodel(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
    "\n",
    "# loss evaluation function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# # optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training the model\n",
    "# num_epochs = 150\n",
    "# patience = 10  # for early stopping\n",
    "# best_loss = np.inf\n",
    "# counter = 0\n",
    "\n",
    "# # initialize lists to store loss values\n",
    "# training_losses = []\n",
    "# validation_losses = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward pass\n",
    "#         outputs = model(X_batch)\n",
    "\n",
    "#         mse_loss = criterion(outputs.squeeze(), y_batch.squeeze())\n",
    "\n",
    "#         # L1 regularization\n",
    "#         l1_loss = l1_regularization(model, lambda_l1)\n",
    "#         l2_loss = l2_regularization(model, lambda_l2)\n",
    "\n",
    "#         # calc total loss\n",
    "#         loss = mse_loss + l1_loss + l2_loss\n",
    "\n",
    "#         # backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "\n",
    "#     # average training loss\n",
    "#     avg_train_loss = train_loss / len(train_loader)\n",
    "#     training_losses.append(avg_train_loss)\n",
    "\n",
    "#     # validation\n",
    "#     model.eval()\n",
    "#     val_losses = []\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in val_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             outputs = model(X_batch)\n",
    "#             mse_loss = criterion(outputs.squeeze(), y_batch.squeeze())\n",
    "#             l1_loss = l1_regularization(model, lambda_l1)\n",
    "#             l2_loss = l2_regularization(model, lambda_l2)\n",
    "#             loss = mse_loss + l1_loss + l2_loss\n",
    "#             val_losses.append(loss.item())\n",
    "\n",
    "#     avg_val_loss = np.mean(val_losses)\n",
    "#     validation_losses.append(avg_val_loss)\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "#     # torch.save(model.state_dict(), 'output/last_model.pth')\n",
    "    \n",
    "#     # early stopping\n",
    "#     if avg_val_loss < best_loss:\n",
    "#         best_loss = avg_val_loss\n",
    "#         counter = 0\n",
    "#         # save the best model\n",
    "#         torch.save(model.state_dict(), 'output/best_model.pth')\n",
    "#     else:\n",
    "#         counter += 1\n",
    "#         if counter >= patience:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/150], Training Loss: 440987.4231, Validation Loss: 109443.9727\n",
      "Epoch [20/150], Training Loss: 249100.4891, Validation Loss: 100407.1340\n",
      "Early stopping after 26 epochs. Best validation loss: 95614.7130\n",
      "Epoch [10/150], Training Loss: 440986.3821, Validation Loss: 109442.9718\n",
      "Epoch [20/150], Training Loss: 250745.3097, Validation Loss: 101347.1441\n",
      "Early stopping after 28 epochs. Best validation loss: 98813.0842\n",
      "Epoch [10/150], Training Loss: 440986.3129, Validation Loss: 109442.0230\n",
      "Epoch [20/150], Training Loss: 250876.0195, Validation Loss: 100801.3406\n",
      "Early stopping after 26 epochs. Best validation loss: 99471.1404\n",
      "Epoch [10/150], Training Loss: 439839.4254, Validation Loss: 108193.2807\n",
      "Epoch [20/150], Training Loss: 249234.0139, Validation Loss: 93495.8340\n",
      "Epoch [30/150], Training Loss: 147434.0315, Validation Loss: 98252.4125\n",
      "Early stopping after 30 epochs. Best validation loss: 91584.9831\n",
      "Epoch [10/150], Training Loss: 439838.4683, Validation Loss: 108191.6982\n",
      "Epoch [20/150], Training Loss: 246444.0071, Validation Loss: 95410.3571\n",
      "Early stopping after 27 epochs. Best validation loss: 91477.2275\n",
      "Epoch [10/150], Training Loss: 439838.3149, Validation Loss: 108191.8420\n",
      "Epoch [20/150], Training Loss: 248074.5270, Validation Loss: 98504.0365\n",
      "Early stopping after 26 epochs. Best validation loss: 96323.7092\n",
      "Epoch [10/150], Training Loss: 397576.3360, Validation Loss: 102701.2083\n",
      "Epoch [20/150], Training Loss: 229724.1081, Validation Loss: 85382.7237\n",
      "Early stopping after 24 epochs. Best validation loss: 83893.9546\n",
      "Epoch [10/150], Training Loss: 387446.5812, Validation Loss: 96739.0995\n",
      "Epoch [20/150], Training Loss: 224531.9828, Validation Loss: 91215.1309\n",
      "Early stopping after 28 epochs. Best validation loss: 86557.3884\n",
      "Epoch [10/150], Training Loss: 390415.1008, Validation Loss: 98232.4472\n",
      "Epoch [20/150], Training Loss: 225258.3575, Validation Loss: 81940.9179\n",
      "Epoch [30/150], Training Loss: 134105.9303, Validation Loss: 87113.5695\n",
      "Early stopping after 29 epochs. Best validation loss: 81940.9179\n",
      "Epoch [10/150], Training Loss: 440991.2869, Validation Loss: 109288.2330\n",
      "Epoch [20/150], Training Loss: 434716.5189, Validation Loss: 106030.7121\n",
      "Epoch [30/150], Training Loss: 433544.7894, Validation Loss: 106116.1882\n",
      "Early stopping after 31 epochs. Best validation loss: 105413.0289\n",
      "Epoch [10/150], Training Loss: 440990.1594, Validation Loss: 109285.8551\n",
      "Epoch [20/150], Training Loss: 434692.7428, Validation Loss: 106150.2262\n",
      "Epoch [30/150], Training Loss: 433553.9557, Validation Loss: 106167.3859\n",
      "Epoch [40/150], Training Loss: 433826.8554, Validation Loss: 105539.3511\n",
      "Epoch [50/150], Training Loss: 433596.6184, Validation Loss: 105533.9073\n",
      "Early stopping after 57 epochs. Best validation loss: 104953.1821\n",
      "Epoch [10/150], Training Loss: 440990.0949, Validation Loss: 109285.4023\n",
      "Epoch [20/150], Training Loss: 434693.5253, Validation Loss: 106151.0161\n",
      "Epoch [30/150], Training Loss: 433493.6694, Validation Loss: 106055.0678\n",
      "Early stopping after 31 epochs. Best validation loss: 105267.7410\n",
      "Epoch [10/150], Training Loss: 439886.6257, Validation Loss: 108121.6937\n",
      "Epoch [20/150], Training Loss: 433379.1440, Validation Loss: 104783.7865\n",
      "Early stopping after 26 epochs. Best validation loss: 104323.3927\n",
      "Epoch [10/150], Training Loss: 439885.7643, Validation Loss: 108120.6339\n",
      "Epoch [20/150], Training Loss: 433350.5781, Validation Loss: 104741.0896\n",
      "Epoch [30/150], Training Loss: 432616.3371, Validation Loss: 105392.6386\n",
      "Early stopping after 31 epochs. Best validation loss: 104326.6070\n",
      "Epoch [10/150], Training Loss: 439885.6454, Validation Loss: 108120.6887\n",
      "Epoch [20/150], Training Loss: 433525.4696, Validation Loss: 104845.6504\n",
      "Early stopping after 26 epochs. Best validation loss: 104371.9136\n",
      "Epoch [10/150], Training Loss: 437598.5728, Validation Loss: 105732.1904\n",
      "Epoch [20/150], Training Loss: 430711.4467, Validation Loss: 102371.6443\n",
      "Early stopping after 25 epochs. Best validation loss: 102142.5381\n",
      "Epoch [10/150], Training Loss: 437597.6035, Validation Loss: 105731.2425\n",
      "Epoch [20/150], Training Loss: 430699.8128, Validation Loss: 102300.0900\n",
      "Early stopping after 24 epochs. Best validation loss: 102132.5747\n",
      "Epoch [10/150], Training Loss: 437597.5110, Validation Loss: 105731.1629\n",
      "Epoch [20/150], Training Loss: 430727.5285, Validation Loss: 102225.8227\n",
      "Early stopping after 24 epochs. Best validation loss: 102149.3795\n",
      "Epoch [10/150], Training Loss: 266432.3040, Validation Loss: 110031.9269\n",
      "Epoch [20/150], Training Loss: 120248.0136, Validation Loss: 106628.2751\n",
      "Early stopping after 23 epochs. Best validation loss: 106206.7276\n",
      "Epoch [10/150], Training Loss: 262384.7328, Validation Loss: 115856.2583\n",
      "Early stopping after 14 epochs. Best validation loss: 110593.4773\n",
      "Epoch [10/150], Training Loss: 264829.0325, Validation Loss: 108554.1730\n",
      "Epoch [20/150], Training Loss: 120624.9955, Validation Loss: 105127.9018\n",
      "Early stopping after 28 epochs. Best validation loss: 102231.2708\n",
      "Epoch [10/150], Training Loss: 258720.4239, Validation Loss: 110432.0641\n",
      "Epoch [20/150], Training Loss: 116158.9280, Validation Loss: 109448.7109\n",
      "Early stopping after 20 epochs. Best validation loss: 104340.7809\n",
      "Epoch [10/150], Training Loss: 257824.1815, Validation Loss: 108749.1930\n",
      "Early stopping after 16 epochs. Best validation loss: 97705.5751\n",
      "Epoch [10/150], Training Loss: 261900.1917, Validation Loss: 97882.1209\n",
      "Early stopping after 17 epochs. Best validation loss: 94079.9170\n",
      "Epoch [10/150], Training Loss: 254345.3161, Validation Loss: 87542.8037\n",
      "Epoch [20/150], Training Loss: 107293.8637, Validation Loss: 84845.9018\n",
      "Early stopping after 20 epochs. Best validation loss: 83581.2382\n",
      "Epoch [10/150], Training Loss: 252070.6010, Validation Loss: 94815.7647\n",
      "Epoch [20/150], Training Loss: 106336.9064, Validation Loss: 86251.6576\n",
      "Early stopping after 25 epochs. Best validation loss: 84706.5088\n",
      "Epoch [10/150], Training Loss: 258043.6903, Validation Loss: 93393.3176\n",
      "Early stopping after 18 epochs. Best validation loss: 85665.5989\n",
      "Epoch [10/150], Training Loss: 439236.6468, Validation Loss: 109026.8470\n",
      "Epoch [20/150], Training Loss: 438596.6277, Validation Loss: 109587.3756\n",
      "Epoch [30/150], Training Loss: 437288.4458, Validation Loss: 109923.7395\n",
      "Early stopping after 37 epochs. Best validation loss: 107743.7643\n",
      "Epoch [10/150], Training Loss: 439379.6420, Validation Loss: 109240.5742\n",
      "Epoch [20/150], Training Loss: 438515.7815, Validation Loss: 109591.1684\n",
      "Epoch [30/150], Training Loss: 437283.3527, Validation Loss: 109644.7833\n",
      "Early stopping after 37 epochs. Best validation loss: 107753.7055\n",
      "Epoch [10/150], Training Loss: 439220.3519, Validation Loss: 109128.4819\n",
      "Epoch [20/150], Training Loss: 438615.4790, Validation Loss: 109687.2657\n",
      "Epoch [30/150], Training Loss: 437232.3920, Validation Loss: 110127.1795\n",
      "Early stopping after 37 epochs. Best validation loss: 107776.4110\n",
      "Epoch [10/150], Training Loss: 436986.8385, Validation Loss: 107419.4366\n",
      "Epoch [20/150], Training Loss: 436286.7534, Validation Loss: 107733.1736\n",
      "Epoch [30/150], Training Loss: 435533.5484, Validation Loss: 107786.1345\n",
      "Early stopping after 31 epochs. Best validation loss: 106074.7237\n",
      "Epoch [10/150], Training Loss: 437107.5115, Validation Loss: 107362.5376\n",
      "Epoch [20/150], Training Loss: 436451.3896, Validation Loss: 107951.2997\n",
      "Epoch [30/150], Training Loss: 435455.1376, Validation Loss: 107867.3864\n",
      "Early stopping after 31 epochs. Best validation loss: 106351.3486\n",
      "Epoch [10/150], Training Loss: 436916.7634, Validation Loss: 107265.2366\n",
      "Epoch [20/150], Training Loss: 436114.6440, Validation Loss: 107963.7598\n",
      "Epoch [30/150], Training Loss: 435521.1141, Validation Loss: 107847.2084\n",
      "Early stopping after 31 epochs. Best validation loss: 106249.7850\n",
      "Epoch [10/150], Training Loss: 333420.2493, Validation Loss: 109055.5134\n",
      "Epoch [20/150], Training Loss: 131846.9459, Validation Loss: 105379.7981\n",
      "Epoch [30/150], Training Loss: 59765.1030, Validation Loss: 100603.8859\n",
      "Early stopping after 33 epochs. Best validation loss: 92648.7455\n",
      "Epoch [10/150], Training Loss: 326570.7184, Validation Loss: 105688.3220\n",
      "Epoch [20/150], Training Loss: 116587.5104, Validation Loss: 87840.1689\n",
      "Early stopping after 26 epochs. Best validation loss: 87402.0417\n",
      "Epoch [10/150], Training Loss: 317742.8960, Validation Loss: 105716.8234\n",
      "Epoch [20/150], Training Loss: 126336.7838, Validation Loss: 96787.6411\n",
      "Epoch [30/150], Training Loss: 57011.3292, Validation Loss: 96166.9118\n",
      "Early stopping after 33 epochs. Best validation loss: 91110.1380\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epochs = 150\n",
    "patience = 10  # for early stopping\n",
    "best_loss_global = np.inf\n",
    "\n",
    "for hidden_dim in hidden_dim_array:\n",
    "    for layer_dim in layer_dim_array:\n",
    "        for lambda_l1 in lambda_l1_array:\n",
    "            for lambda_l2 in lambda_l2_array:\n",
    "                torch.manual_seed(2024)\n",
    "                np.random.seed(2024)\n",
    "                model = LSTMmodel(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                # set seed for reproducibility\n",
    "                best_loss = np.inf\n",
    "                counter = 0\n",
    "\n",
    "                # initialize lists to store loss values\n",
    "                training_losses = []\n",
    "                validation_losses = []\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    train_loss = 0\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # forward pass\n",
    "                        outputs = model(X_batch)\n",
    "\n",
    "                        mse_loss = criterion(outputs.squeeze(), y_batch.squeeze())\n",
    "\n",
    "                        # L1 regularization\n",
    "                        l1_loss = l1_regularization(model, lambda_l1)\n",
    "                        l2_loss = l2_regularization(model, lambda_l2)\n",
    "\n",
    "                        # calc total loss\n",
    "                        loss = mse_loss + l1_loss + l2_loss\n",
    "\n",
    "                        # backward pass and optimization\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        train_loss += loss.item()\n",
    "\n",
    "\n",
    "                    # average training loss\n",
    "                    avg_train_loss = train_loss / len(train_loader)\n",
    "                    training_losses.append(avg_train_loss)\n",
    "\n",
    "                    # validation\n",
    "                    model.eval()\n",
    "                    val_losses = []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            outputs = model(X_batch)\n",
    "                            mse_loss = criterion(outputs.squeeze(), y_batch.squeeze())\n",
    "                            l1_loss = l1_regularization(model, lambda_l1)\n",
    "                            l2_loss = l2_regularization(model, lambda_l2)\n",
    "                            loss = mse_loss + l1_loss + l2_loss\n",
    "                            val_losses.append(loss.item())\n",
    "\n",
    "                    avg_val_loss = np.mean(val_losses)\n",
    "                    validation_losses.append(avg_val_loss)\n",
    "                    if (epoch + 1) % 10 == 0:\n",
    "                        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "                    # torch.save(model.state_dict(), 'output/last_model.pth')\n",
    "                    \n",
    "                    # early stopping\n",
    "                    if avg_val_loss < best_loss:\n",
    "                        best_loss = avg_val_loss\n",
    "                        counter = 0\n",
    "                        # save the best model\n",
    "                        torch.save(model.state_dict(), f'output/best_model_{lambda_l1_array.index(lambda_l1)}_{lambda_l2_array.index(lambda_l2)}_{hidden_dim_array.index(hidden_dim)}_{layer_dim_array.index(layer_dim)}.pth')\n",
    "                        if best_loss < best_loss_global:\n",
    "                            best_loss_global = best_loss\n",
    "                            torch.save(model.state_dict(), f'output/best_model_global.pth')\n",
    "                    else:\n",
    "                        counter += 1\n",
    "                        if counter >= patience:\n",
    "                            print(f\"Early stopping after {epoch} epochs. Best validation loss: {best_loss:.4f}\")\n",
    "                            break\n",
    "                    \n",
    "                    if epoch == (num_epochs - 1):\n",
    "                        print(f\"Training stopped after {epoch} epochs. Best validation loss: {best_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden dim:  256, Layer dim: 2,  L1:    1.5,  L2:  0.001,    Validation Loss:     95614.7130\n",
      "RMSE: 389.1576, MAE: 298.3920\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    1.5,  L2:  0.0001,    Validation Loss:     98813.0842\n",
      "RMSE: 375.5853, MAE: 290.5905\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    1.5,  L2:  1e-05,    Validation Loss:     99471.1404\n",
      "RMSE: 397.0325, MAE: 306.9723\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    1,  L2:  0.001,    Validation Loss:     91584.9831\n",
      "RMSE: 384.4730, MAE: 300.2742\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    1,  L2:  0.0001,    Validation Loss:     91477.2275\n",
      "RMSE: 373.8942, MAE: 287.8629\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    1,  L2:  1e-05,    Validation Loss:     96323.7092\n",
      "RMSE: 400.8442, MAE: 311.9087\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    0.1,  L2:  0.001,    Validation Loss:     83893.9546\n",
      "RMSE: 371.8143, MAE: 283.5408\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    0.1,  L2:  0.0001,    Validation Loss:     86557.3884\n",
      "RMSE: 382.8279, MAE: 297.9919\n",
      "Hidden dim:  256, Layer dim: 2,  L1:    0.1,  L2:  1e-05,    Validation Loss:     81940.9179\n",
      "RMSE: 382.7634, MAE: 295.4201\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    1.5,  L2:  0.001,    Validation Loss:     105413.0289\n",
      "RMSE: 401.8344, MAE: 311.8475\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    1.5,  L2:  0.0001,    Validation Loss:     104953.1821\n",
      "RMSE: 400.1212, MAE: 309.9053\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    1.5,  L2:  1e-05,    Validation Loss:     105267.7410\n",
      "RMSE: 401.6600, MAE: 311.6548\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    1,  L2:  0.001,    Validation Loss:     104323.3927\n",
      "RMSE: 400.0676, MAE: 309.8419\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    1,  L2:  0.0001,    Validation Loss:     104326.6070\n",
      "RMSE: 401.4539, MAE: 311.4266\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    1,  L2:  1e-05,    Validation Loss:     104371.9136\n",
      "RMSE: 400.0632, MAE: 309.8367\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    0.1,  L2:  0.001,    Validation Loss:     102142.5381\n",
      "RMSE: 399.9814, MAE: 309.7396\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    0.1,  L2:  0.0001,    Validation Loss:     102132.5747\n",
      "RMSE: 399.5316, MAE: 309.2002\n",
      "Hidden dim:  256, Layer dim: 3,  L1:    0.1,  L2:  1e-05,    Validation Loss:     102149.3795\n",
      "RMSE: 400.5007, MAE: 310.3473\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    1.5,  L2:  0.001,    Validation Loss:     106206.7276\n",
      "RMSE: 418.6622, MAE: 321.3165\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    1.5,  L2:  0.0001,    Validation Loss:     110593.4773\n",
      "RMSE: 414.5573, MAE: 321.8184\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    1.5,  L2:  1e-05,    Validation Loss:     102231.2708\n",
      "RMSE: 396.0673, MAE: 306.9854\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    1,  L2:  0.001,    Validation Loss:     104340.7809\n",
      "RMSE: 393.9948, MAE: 303.7370\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    1,  L2:  0.0001,    Validation Loss:     97705.5751\n",
      "RMSE: 390.0547, MAE: 304.8654\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    1,  L2:  1e-05,    Validation Loss:     94079.9170\n",
      "RMSE: 372.0885, MAE: 283.4508\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    0.1,  L2:  0.001,    Validation Loss:     83581.2382\n",
      "RMSE: 361.7701, MAE: 279.1287\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    0.1,  L2:  0.0001,    Validation Loss:     84706.5088\n",
      "RMSE: 357.4210, MAE: 277.1670\n",
      "Hidden dim:  512, Layer dim: 2,  L1:    0.1,  L2:  1e-05,    Validation Loss:     85665.5989\n",
      "RMSE: 382.6041, MAE: 295.5481\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    1.5,  L2:  0.001,    Validation Loss:     107743.7643\n",
      "RMSE: 399.3012, MAE: 308.9191\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    1.5,  L2:  0.0001,    Validation Loss:     107753.7055\n",
      "RMSE: 398.9645, MAE: 308.5028\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    1.5,  L2:  1e-05,    Validation Loss:     107776.4110\n",
      "RMSE: 399.0599, MAE: 308.6217\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    1,  L2:  0.001,    Validation Loss:     106074.7237\n",
      "RMSE: 400.4253, MAE: 310.2601\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    1,  L2:  0.0001,    Validation Loss:     106351.3486\n",
      "RMSE: 399.8693, MAE: 309.6062\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    1,  L2:  1e-05,    Validation Loss:     106249.7850\n",
      "RMSE: 399.4420, MAE: 309.0912\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    0.1,  L2:  0.001,    Validation Loss:     92648.7455\n",
      "RMSE: 361.2173, MAE: 273.0669\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    0.1,  L2:  0.0001,    Validation Loss:     87402.0417\n",
      "RMSE: 356.6261, MAE: 275.2010\n",
      "Hidden dim:  512, Layer dim: 3,  L1:    0.1,  L2:  1e-05,    Validation Loss:     91110.1380\n",
      "RMSE: 365.3430, MAE: 282.7675\n",
      "Best: \n",
      " Hidden dim:  256, Layer dim: 2,  L1:    0.1,  L2:  1e-05,    Best Validation Loss:     81940.9179\n"
     ]
    }
   ],
   "source": [
    "# load in the all the models and print the best validation loss\n",
    "best_val_loss = np.inf\n",
    "for hidden_dim in hidden_dim_array:\n",
    "    for layer_dim in layer_dim_array:\n",
    "        for lambda_l1 in lambda_l1_array:\n",
    "            for lambda_l2 in lambda_l2_array:\n",
    "                model = LSTMmodel(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
    "                model.load_state_dict(torch.load(f'output/best_model_{lambda_l1_array.index(lambda_l1)}_{lambda_l2_array.index(lambda_l2)}_{hidden_dim_array.index(hidden_dim)}_{layer_dim_array.index(layer_dim)}.pth'))\n",
    "                model.eval()\n",
    "                val_losses = []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in val_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        outputs = model(X_batch)\n",
    "                        mse_loss = criterion(outputs.squeeze(), y_batch.squeeze())\n",
    "                        l1_loss = l1_regularization(model, lambda_l1)\n",
    "                        l2_loss = l2_regularization(model, lambda_l2)\n",
    "                        loss = mse_loss + l1_loss + l2_loss\n",
    "                        val_losses.append(loss.item())\n",
    "\n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "                predictions = []\n",
    "                actuals = []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in test_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        outputs = model(X_batch)\n",
    "                        predictions.extend(outputs.squeeze().tolist())\n",
    "                        actuals.extend(y_batch.tolist())\n",
    "                rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "                mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    best_hidden_dim = hidden_dim\n",
    "                    best_lambda_l1 = lambda_l1\n",
    "                    best_lambda_l2 = lambda_l2\n",
    "                    best_layer_dim = layer_dim\n",
    "                print(f'Hidden dim:  {hidden_dim}, Layer dim: {layer_dim},  L1:    {lambda_l1},  L2:  {lambda_l2},    Validation Loss:     {avg_val_loss:.4f}')\n",
    "                print(f'RMSE: {rmse:.4f}, MAE: {mae:.4f}')\n",
    "\n",
    "print(f'Best: \\n Hidden dim:  {best_hidden_dim}, Layer dim: {best_layer_dim},  L1:    {best_lambda_l1},  L2:  {best_lambda_l2},    Best Validation Loss:     {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses = np.array(training_losses)/1000\n",
    "# val_losses = np.array(validation_losses)/1000\n",
    "\n",
    "# # Plotting the loss curves\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "# plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Avg loss (x1,000)')\n",
    "# # plt.yscale('log')\n",
    "# plt.title('Training and Validation Loss Over Epochs')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current best hyperparameters\n",
    "# opt_hidden_dim = 512\n",
    "# opt_lambda_l1 = 0.1\n",
    "# opt_lambda_l2 = 1e-5\n",
    "# opt_layer_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the best model\n",
    "# model = LSTMmodel(input_dim, best_hidden_dim, best_layer_dim, output_dim).to(device)\n",
    "# model.load_state_dict(torch.load(f'output/best_model_{lambda_l1_array.index(best_lambda_l1)}_{lambda_l2_array.index(best_lambda_l2)}_{hidden_dim_array.index(best_hidden_dim)}.pth'))\n",
    "# # model.load_state_dict(torch.load('output/best_model.pth'))\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = []\n",
    "#     actuals = []\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         outputs = model(X_batch)\n",
    "#         predictions.extend(outputs.squeeze().tolist())\n",
    "#         #\n",
    "\n",
    "#         actuals.extend(y_batch.tolist())\n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "# mae = mean_absolute_error(actuals, predictions)\n",
    "# r2 = r2_score(actuals, predictions)\n",
    "\n",
    "# print(f'Performance on test data:\\nRMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nR^2 Score: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert 'predictions' and 'actuals' to numpy arrays\n",
    "# predictions = np.array(predictions)\n",
    "# actuals = np.array(actuals)\n",
    "\n",
    "# # extract 'from' timestamps from test_data, adjusted for seq_length\n",
    "# # since the dataset uses sequences, the first 'seq_length' targets are not included in 'actuals' and 'predictions'\n",
    "# test_timestamps = test_data['from'].values[seq_length:]\n",
    "\n",
    "# # ensure lengths match\n",
    "# min_length = min(len(test_timestamps), len(actuals), len(predictions))\n",
    "# test_timestamps = test_timestamps[:min_length]\n",
    "# actuals = actuals[:min_length]\n",
    "# predictions = predictions[:min_length]\n",
    "\n",
    "# # create the plot\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.plot(test_timestamps, actuals, label='Actual Prices', color='blue', alpha=0.7)\n",
    "# plt.plot(test_timestamps, predictions, label='Predicted Prices', color='red', alpha=0.7)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('Electricity Price')\n",
    "# plt.title('Predicted vs. Actual Electricity Prices on Test Data')\n",
    "\n",
    "# # format x-axis with date labels\n",
    "# plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig('output/lstm_predicted_vs_actual_prices2024.png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the first 500 data points\n",
    "# datapoints = 24*21\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.plot(test_timestamps[:datapoints], actuals[:datapoints], label='Actual Prices', color='blue', alpha=0.7)\n",
    "# plt.plot(test_timestamps[:datapoints], predictions[:datapoints], label='Predicted Prices', color='red', alpha=0.7)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('Electricity Price')\n",
    "# plt.title(f'Predicted vs. Actual Electricity Prices (Last {datapoints//24} Days)')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(f'output/lstm_predicted_vs_actual_prices_last{datapoints//24}days.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.scatter(actuals, predictions, alpha=0.5)\n",
    "# plt.xlabel('Actual Prices')\n",
    "# plt.ylabel('Predicted Prices')\n",
    "# plt.title('Predicted vs. Actual Electricity Prices')\n",
    "\n",
    "# # Plot a diagonal line for reference\n",
    "# min_price = min(actuals.min(), predictions.min())\n",
    "# max_price = max(actuals.max(), predictions.max())\n",
    "# plt.plot([min_price, max_price], [min_price, max_price], 'k--', lw=2)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig('output/lstm_predicted_vs_actual_prices_scatter.png')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
